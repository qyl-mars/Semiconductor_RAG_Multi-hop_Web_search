import sys
import os
import time

# --- 1. ç¯å¢ƒé…ç½® (ç¡®ä¿ Python èƒ½æ‰¾åˆ°ä½ çš„æ¨¡å—) ---
# è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path ä¸­ (å‡è®¾ä½ åœ¨æ ¹ç›®å½•è¿è¡Œï¼Œè¿™ä¸€æ­¥æ˜¯åŒä¿é™©)
if current_dir not in sys.path:
    sys.path.append(current_dir)

try:
    from search.web_search import get_web_search_content, search_bing, web_search_and_rerank
except ImportError as e:
    print("âŒ å¯¼å…¥æ¨¡å—å¤±è´¥ï¼è¯·ç¡®ä¿ä½ åœ¨é¡¹ç›®æ ¹ç›®å½• 'Rag/' ä¸‹è¿è¡Œæ­¤è„šæœ¬ã€‚")
    print(f"é”™è¯¯è¯¦æƒ…: {e}")
    sys.exit(1)


def test_connectivity():
    """æµ‹è¯•åŸºç¡€ç½‘ç»œè¿æ¥"""
    print("\n[1/3] æ­£åœ¨æµ‹è¯•åŸºç¡€ç½‘ç»œè¿æ¥ (Ping cn.bing.com)...")
    try:
        import requests
        resp = requests.get("https://cn.bing.com", timeout=5)
        if resp.status_code == 200:
            print(f"âœ… ç½‘ç»œé€šç•…ï¼ŒçŠ¶æ€ç : {resp.status_code}")
            return True
        else:
            print(f"âš ï¸ æ— æ³•è®¿é—® Bingï¼ŒçŠ¶æ€ç : {resp.status_code}")
            return False
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¿æ¥å¤±è´¥: {e}")
        return False


def test_raw_crawler(query):
    """å•ç‹¬æµ‹è¯•çˆ¬è™«å‡½æ•° (search_bing)ï¼Œæ’é™¤æ’åºç®—æ³•çš„å¹²æ‰°"""
    print(f"\n[2/3] æ­£åœ¨æµ‹è¯•åº•å±‚çˆ¬è™« search_bing('{query}')...")
    print("      (å¦‚æœè¿™ä¸€æ­¥å¤±è´¥ï¼Œè¯´æ˜æ˜¯ Cookie è¿‡æœŸäº†)")

    start_time = time.time()
    results = search_bing(query)
    end_time = time.time()

    if results and len(results) > 0:
        print(f"âœ… çˆ¬è™«æˆåŠŸï¼æŠ“å–åˆ° {len(results)} æ¡ç»“æœã€‚")
        print(f"      è€—æ—¶: {end_time - start_time:.2f} ç§’")
        print(f"      ç¬¬ä¸€æ¡æ ‡é¢˜: {results[0].get('title', 'æ— æ ‡é¢˜')}")
        print(f"      ç¬¬ä¸€æ¡é“¾æ¥: {results[0].get('url', 'æ— é“¾æ¥')}")
        return True
    else:
        print("âŒ çˆ¬è™«è¿”å›ç»“æœä¸ºç©ºã€‚")
        print("ğŸ’¡ å»ºè®®ï¼šè¯·æ›´æ–° web_search.py ä¸­çš„ Cookieã€‚")
        return False


def test_full_pipeline(query):
    """æµ‹è¯•å®Œæ•´æµç¨‹ (çˆ¬è™« + æ’åº + æ¸…æ´—)"""
    print(f"\n[3/3] æ­£åœ¨æµ‹è¯•å®Œæ•´æµç¨‹ get_web_search_content('{query}')...")

    start_time = time.time()
    final_content = get_web_search_content(query, max_length=500)
    end_time = time.time()

    if final_content:
        print(f"âœ… å®Œæ•´æµç¨‹æˆåŠŸï¼")
        print(f"      è€—æ—¶: {end_time - start_time:.2f} ç§’")
        print(f"      è¿”å›å†…å®¹é•¿åº¦: {len(final_content)} å­—ç¬¦")
        print("-" * 30)
        print("é¢„è§ˆå†…å®¹ (å‰ 200 å­—):")
        print(final_content[:200] + "...")
        print("-" * 30)
    else:
        print("âŒ å®Œæ•´æµç¨‹å¤±è´¥ï¼Œè¿”å›å†…å®¹ä¸ºç©ºã€‚")


if __name__ == "__main__":
    test_query = "ç›®å‰æœ€å…ˆè¿›çš„åŠå¯¼ä½“å…‰åˆ»æœºæ˜¯å“ªä¸ªå…¬å¸çš„"

    # æŒ‰é¡ºåºæ‰§è¡Œæµ‹è¯•
    if test_connectivity():
        if test_raw_crawler(test_query):
            test_full_pipeline(test_query)
        else:
            print("\nâ›” æµ‹è¯•ç»ˆæ­¢ï¼šåº•å±‚çˆ¬è™«å¤±è´¥ï¼Œæ— éœ€æµ‹è¯•åç»­æµç¨‹ã€‚")
    else:
        print("\nâ›” æµ‹è¯•ç»ˆæ­¢ï¼šç½‘ç»œä¸é€šã€‚")